# ğŸ” Google CSE SERP Extractor

[![Python](https://img.shields.io/badge/Python-3.8+-blue?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge&logo=opensourceinitiative&logoColor=white)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen?style=for-the-badge)]()

A **production-ready, enterprise-grade** Google Search Engine Results Page (SERP) data extraction tool that leverages Google's Custom Search Engine API to scrape search results at scale. Built with modern Python technologies, this tool provides both a powerful CLI interface and an intuitive web application for bulk keyword research, competitive analysis, and SEO data collection.

---

## ğŸ“‹ Description

The Google CSE SERP Extractor is designed for **digital marketers, SEO professionals, data analysts, and researchers** who need reliable, scalable access to Google search results data. Unlike web scraping approaches that risk IP blocking and rate limiting, this tool uses Google's official Custom Search Engine API to deliver **consistent, accurate, and compliant** search data extraction.

**Key Value Propositions:**
- **100% API-based approach** eliminates IP blocking risks and ensures data accuracy
- **Bulk processing capabilities** handle thousands of keywords efficiently with intelligent rate limiting
- **Multi-format support** for input (CSV, Excel, JSON) and output (JSON, CSV, Excel) with seamless data transformation
- **Advanced filtering options** including profile site targeting, query normalization, and duplicate detection
- **Production-grade architecture** with comprehensive error handling, logging, and quota management

The tool processes **10-100 results per keyword** across **1-3 pages** with configurable pagination, delivering structured data that includes titles, URLs, snippets, positions, and metadata. Built-in caching mechanisms and intelligent delay systems ensure optimal API usage while maintaining high throughput for large-scale data collection projects.

---

## âœ¨ Key Benefits and Features

### ğŸš€ **High-Performance Data Extraction**
Process thousands of keywords efficiently with intelligent batching, automatic retry mechanisms, and smart rate limiting. The tool handles up to **100 results per keyword** across multiple pages, delivering comprehensive search result datasets in minutes rather than hours.

### ğŸ¯ **Advanced Query Targeting**
Target specific platforms and domains using intelligent site filtering. The built-in platform suggestion system supports **50+ popular platforms** (LinkedIn, GitHub, Twitter, etc.) with automatic query transformation, enabling precise profile and content discovery across professional networks.

### ğŸ“Š **Flexible Data Management**
Support for multiple input formats (CSV, Excel, JSON) and output formats (JSON, CSV, Excel) with automatic data validation and transformation. The unified schema ensures consistent data structure regardless of input source, making integration with existing workflows seamless.

### ğŸ”§ **Production-Ready Architecture**
Enterprise-grade error handling, comprehensive logging, and quota management ensure reliable operation at scale. Built-in caching reduces API costs, while automatic cleanup and resource management prevent system resource exhaustion during long-running operations.

### ğŸŒ **Dual Interface Design**
Choose between a powerful CLI for automation and scripting, or an intuitive web interface for interactive use. Both interfaces share the same robust backend, ensuring consistent results regardless of your preferred workflow.

### ğŸ”’ **Secure Token Management**
Advanced token management with backup API key support, automatic quota detection, and secure temporary storage. The system automatically switches to backup tokens when quota limits are reached, ensuring uninterrupted operation.

---

## ğŸš€ Installation

### Prerequisites
- **Python 3.8+** (Python 3.9+ recommended for optimal performance)
- **Google Cloud Console** account with Custom Search Engine API enabled
- **Google Custom Search Engine** configured with appropriate search settings

### Quick Setup (macOS)

```bash
# Clone the repository
git clone https://github.com/NgnPhamGiaHuy/google-cse-serp-extractor.git
cd google-cse-serp-extractor

# Run the automated setup script
chmod +x setup.sh
./setup.sh
```

The setup script will:
- âœ… Check Python version and install if needed
- âœ… Create a virtual environment
- âœ… Install all dependencies
- âœ… Generate configuration files
- âœ… Test the installation

### Manual Installation

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create configuration file
cp env.example .env
```

### Environment Configuration

Create a `.env` file with your Google API credentials:

```bash
# Google Custom Search Engine API
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_CSE_CX=your_custom_search_engine_id_here

# Optional: Backup API keys for quota management
GOOGLE_API_KEY_BACKUP=your_backup_api_key_here
GOOGLE_CX_BACKUP=your_backup_cse_id_here
```

---

## ğŸ’» Usage

### Web Interface (Recommended)

Start the web application using the convenient launcher script:

```bash
# Using the launcher script (recommended)
./run.sh

# Or manually
python app.py
```

The launcher script will:
- âœ… Check for virtual environment and dependencies
- âœ… Validate API key configuration
- âœ… Find an available port if 8000 is occupied
- âœ… Start the web server with proper error handling

Navigate to `http://localhost:8000` in your browser for the interactive interface.

**Web Interface Features:**
- ğŸ“ **File Upload**: Drag-and-drop support for CSV, Excel, and JSON files
- âŒ¨ï¸ **Manual Queries**: Direct text input with real-time preview
- ğŸ¯ **Platform Targeting**: Visual platform selection with auto-suggestions
- ğŸ“Š **Real-time Progress**: Live progress tracking with detailed statistics
- ğŸ“ˆ **Results Visualization**: Interactive results display with filtering
- ğŸ’¾ **Export Options**: One-click export to JSON, CSV, or Excel formats

### Command Line Interface

#### Basic Usage

```bash
# Single keyword search
python cli.py --keywords "python web scraping" --output results.json

# Multiple keywords from file
python cli.py --keywords keywords.csv --output results.xlsx --max-pages 3

# Advanced filtering with platform targeting
python cli.py --keywords keywords.json --output results.csv \
  --profile-site "site:linkedin.com/in" \
  --profile-site "site:github.com" \
  --results-per-page 50
```

#### Advanced Options

```bash
# Full configuration example
python cli.py \
  --keywords data/keywords.xlsx \
  --output results/search_results_$(date +%Y%m%d).json \
  --max-pages 3 \
  --results-per-page 100 \
  --include-organic \
  --profile-site "site:linkedin.com/in" \
  --profile-site "site:twitter.com" \
  --allow-duplicates
```

### Configuration Options

| Parameter | Description | Default | Range |
|-----------|-------------|---------|-------|
| `--max-pages` | Maximum pages to scrape per keyword | 3 | 1-10 |
| `--results-per-page` | Results per page | 10 | 10-100 |
| `--include-organic` | Include organic search results | True | Boolean |
| `--include-paa` | Include "People Also Ask" results | False | Boolean |
| `--include-related` | Include related search suggestions | False | Boolean |
| `--include-ads` | Include advertisement results | False | Boolean |
| `--profile-site` | Target specific sites (can be used multiple times) | None | String |
| `--allow-duplicates` | Allow duplicate queries after normalization | False | Boolean |

---

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Required | Example |
|----------|-------------|----------|---------|
| `GOOGLE_API_KEY` | Google Custom Search Engine API key | Yes | `AIzaSyB...` |
| `GOOGLE_CSE_CX` | Custom Search Engine ID | Yes | `012345678901234567890:abcdefghijk` |
| `GOOGLE_API_KEY_BACKUP` | Backup API key for quota management | No | `AIzaSyB...` |
| `GOOGLE_CX_BACKUP` | Backup CSE ID | No | `012345678901234567890:xyzabcdef` |

### Configuration File (`config.yaml`)

The tool uses a centralized YAML configuration file for advanced settings:

```yaml
api:
  provider_order: [google]
  google:
    base_url: https://www.googleapis.com/customsearch/v1
    apply_locale_hints: false

search:
  results_per_page: 10
  max_pages: 3
  country: null
  language: null
  device: desktop

behavior:
  max_retries: 2
  delay_ms: 1000
  fallback_strategy: sequential

caching:
  enabled: true
  ttl_seconds: 86400
```

---

## ğŸ“ Folder Structure

```
google-cse-serp-extractor/
â”œâ”€â”€ ğŸ“ serp_tool/                    # Core application package
â”‚   â”œâ”€â”€ ğŸ“ cli/                      # Command-line interface
â”‚   â”‚   â”œâ”€â”€ main.py                  # CLI entry point and commands
â”‚   â”‚   â””â”€â”€ helpers.py               # CLI utility functions
â”‚   â”œâ”€â”€ ğŸ“ clients/                  # API client implementations
â”‚   â”‚   â””â”€â”€ ğŸ“ cse/                  # Google CSE client
â”‚   â”‚       â”œâ”€â”€ client.py            # Main CSE API client
â”‚   â”‚       â”œâ”€â”€ cache.py             # Response caching system
â”‚   â”‚       â”œâ”€â”€ http.py              # HTTP request handling
â”‚   â”‚       â””â”€â”€ mapping.py           # Data transformation utilities
â”‚   â”œâ”€â”€ ğŸ“ handlers/                 # Data processing handlers
â”‚   â”‚   â”œâ”€â”€ readers.py               # Input file readers (CSV, Excel, JSON)
â”‚   â”‚   â”œâ”€â”€ writers.py               # Output file writers
â”‚   â”‚   â”œâ”€â”€ flatteners.py            # Data structure flattening
â”‚   â”‚   â””â”€â”€ common.py                # Shared handler utilities
â”‚   â”œâ”€â”€ ğŸ“ web/                      # Web application
â”‚   â”‚   â”œâ”€â”€ app.py                   # FastAPI application setup
â”‚   â”‚   â”œâ”€â”€ routes.py                # API endpoints and routing
â”‚   â”‚   â”œâ”€â”€ background.py            # Background job processing
â”‚   â”‚   â”œâ”€â”€ state.py                 # Application state management
â”‚   â”‚   â””â”€â”€ helpers.py               # Web utility functions
â”‚   â”œâ”€â”€ ğŸ“ utils/                    # Utility modules
â”‚   â”‚   â”œâ”€â”€ query_utils.py           # Query processing and validation
â”‚   â”‚   â”œâ”€â”€ dedup_utils.py           # Duplicate detection and removal
â”‚   â”‚   â”œâ”€â”€ platform_utils.py        # Platform name to site filter conversion
â”‚   â”‚   â”œâ”€â”€ token_manager.py         # API token management
â”‚   â”‚   â”œâ”€â”€ temp_manager.py          # Temporary file management
â”‚   â”‚   â””â”€â”€ delay_utils.py           # Rate limiting and delays
â”‚   â””â”€â”€ ğŸ“ logging/                  # Logging system
â”‚       â”œâ”€â”€ core.py                  # Logging configuration
â”‚       â”œâ”€â”€ formatter.py             # Log message formatting
â”‚       â””â”€â”€ filters.py               # Log filtering utilities
â”œâ”€â”€ ğŸ“ models/                       # Data models and schemas
â”‚   â”œâ”€â”€ entities.py                  # Core data entities
â”‚   â”œâ”€â”€ search_config.py             # Search configuration models
â”‚   â””â”€â”€ job_status.py                # Job status tracking models
â”œâ”€â”€ ğŸ“ config/                       # Configuration management
â”‚   â””â”€â”€ core.py                      # Configuration loading and validation
â”œâ”€â”€ ğŸ“ templates/                    # Web interface templates
â”‚   â”œâ”€â”€ base.html                    # Base HTML template
â”‚   â””â”€â”€ index.html                   # Main application interface
â”œâ”€â”€ ğŸ“ static/                       # Static web assets
â”‚   â””â”€â”€ ğŸ“ css/                      # Stylesheets
â”œâ”€â”€ ğŸ“ downloads/                    # Output file directory
â”œâ”€â”€ ğŸ“ logs/                         # Application logs
â”œâ”€â”€ ğŸ“„ app.py                        # Web application entry point
â”œâ”€â”€ ğŸ“„ cli.py                        # CLI entry point
â”œâ”€â”€ ğŸ“„ requirements.txt              # Python dependencies
â”œâ”€â”€ ğŸ“„ config.yaml                   # Application configuration
â””â”€â”€ ğŸ“„ schema.py                     # Type definitions and schemas
```

---

## ğŸ¤ Contributing

We welcome contributions from the community! Here's how you can get started:

### Development Setup

```bash
# Fork and clone the repository
git clone https://github.com/your-username/google-cse-serp-extractor.git
cd google-cse-serp-extractor

# Create a development environment
python3 -m venv venv
source venv/bin/activate

# Install development dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # If available

# Install pre-commit hooks
pre-commit install
```

### Contribution Workflow

1. **Create a feature branch** from `main`:
   ```bash
   git checkout -b feature/your-feature-name
   ```

2. **Make your changes** following the existing code style and patterns

3. **Add tests** for new functionality (if applicable)

4. **Run the test suite**:
   ```bash
   python -m pytest tests/
   ```

5. **Commit your changes** with descriptive commit messages:
   ```bash
   git add .
   git commit -m "feat: add new platform targeting feature"
   ```

6. **Push to your fork** and create a pull request:
   ```bash
   git push origin feature/your-feature-name
   ```

### Code Style Guidelines

- Follow **PEP 8** Python style guidelines
- Use **type hints** for all function parameters and return values
- Write **comprehensive docstrings** for all public functions and classes
- Maintain **test coverage** above 80% for new features
- Use **meaningful variable names** and avoid abbreviations

### Reporting Issues

When reporting bugs or requesting features, please include:
- **Clear description** of the issue or feature request
- **Steps to reproduce** (for bugs)
- **Expected vs. actual behavior**
- **Environment details** (Python version, OS, etc.)
- **Log files** (if applicable)

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

The MIT License allows you to:
- âœ… Use the software commercially
- âœ… Modify and distribute the software
- âœ… Include the software in proprietary applications
- âœ… Use the software privately

---

## ğŸ‘¤ Author

<div align="center">

**NgnPhamGiaHuy**

[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/NgnPhamGiaHuy)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://linkedin.com/in/nguyenphamgiahuy)

</div>

---

## ğŸ™ Acknowledgements

Special thanks to the following projects and communities that made this tool possible:

- **[FastAPI](https://fastapi.tiangolo.com/)** - Modern, fast web framework for building APIs
- **[Google Custom Search Engine API](https://developers.google.com/custom-search/v1/introduction)** - Official Google search API
- **[Pandas](https://pandas.pydata.org/)** - Powerful data manipulation and analysis library
- **[Click](https://click.palletsprojects.com/)** - Command-line interface creation toolkit
- **[Pydantic](https://pydantic-docs.helpmanual.io/)** - Data validation using Python type annotations

---

<div align="center">

**â­ Star this repository if you find it helpful!**

[Report Bug](https://github.com/NgnPhamGiaHuy/google-cse-serp-extractor/issues) Â· [Request Feature](https://github.com/NgnPhamGiaHuy/google-cse-serp-extractor/issues) Â· [View Documentation](https://github.com/NgnPhamGiaHuy/google-cse-serp-extractor/wiki)

</div>